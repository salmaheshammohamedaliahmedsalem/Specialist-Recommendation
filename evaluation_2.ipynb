{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TEwNmnTXm0uc"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "69cb9dc23ce948ffa4710126581076fe",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py:898: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
            "  warnings.warn(\n",
            "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:476: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b496a6a2a49e49abb38a6279de2d9ce4",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Device set to use cuda:0\n",
            "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n",
            "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
            "  warnings.warn(\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Evaluation complete for the first 30 rows. Overall specialist accuracy: 65\n",
            "✅ Average question quality score: 6 \n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM\n",
        "import torch\n",
        "from huggingface_hub import login\n",
        "\n",
        "# ✅ Login if needed (Hugging Face API token)\n",
        "login()\n",
        "\n",
        "# Load model and tokenizer for the medical expert LLM\n",
        "model_name = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, use_auth_token=True)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"auto\", torch_dtype=\"auto\", use_auth_token=True)\n",
        "\n",
        "llm = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
        "\n",
        "# Function to assess question quality\n",
        "def evaluate_question_quality(dialogue):\n",
        "    prompt = f\"\"\"\n",
        "    You are a senior medical expert. Based on the following patient-doctor dialogue, rate the quality of the questions asked:\n",
        "    \"{dialogue}\"\n",
        "    The quality of the questions should be evaluated based on their clarity, relevance, and how well they help in diagnosing the patient.\n",
        "    Provide a score from 1 (poor) to 10 (excellent) and a brief explanation.\n",
        "    \"\"\"\n",
        "    response = llm(prompt, max_new_tokens=100, do_sample=False)[0]['generated_text']\n",
        "    return response.strip()\n",
        "\n",
        "# Function to assess the correctness of the assigned specialist\n",
        "def evaluate_specialist(dialogue, assigned_specialist):\n",
        "    prompt = f\"\"\"\n",
        "    You are a senior medical expert. Given the following patient-doctor dialogue, assess if the assigned specialist is correct:\n",
        "    Dialogue: \"{dialogue}\"\n",
        "    Assigned Specialist: {assigned_specialist}\n",
        "    Based on the patient's condition described in the dialogue, is this the correct specialist to be assigned? Answer with \"Correct\" or \"Incorrect\" and provide a brief explanation.\n",
        "    \"\"\"\n",
        "    response = llm(prompt, max_new_tokens=100, do_sample=False)[0]['generated_text']\n",
        "    return response.strip()\n",
        "\n",
        "# Function to compute evaluation score for each case\n",
        "def evaluate_case(row):\n",
        "    dialogue = row['dialogue']\n",
        "    assigned_specialist = row['specialist_label']\n",
        "    \n",
        "    # Evaluate the question quality and specialist correctness\n",
        "    question_quality = evaluate_question_quality(dialogue)\n",
        "    specialist_evaluation = evaluate_specialist(dialogue, assigned_specialist)\n",
        "    \n",
        "    # Generate a score for each\n",
        "    question_score = int(question_quality.split()[0]) if question_quality.split()[0].isdigit() else 0\n",
        "    specialist_score = 1 if \"Correct\" in specialist_evaluation else 0\n",
        "    \n",
        "    # Return the results\n",
        "    return question_score, specialist_score, question_quality, specialist_evaluation\n",
        "\n",
        "# Load the dataset containing the cases (only first 30 rows)\n",
        "df = pd.read_csv(\"/teamspace/studios/this_studio/labeled_data.csv\").head(30)\n",
        "\n",
        "# Apply the evaluation function\n",
        "df[['Question Quality Score', 'Specialist Score', 'Question Quality Explanation', 'Specialist Evaluation']] = df.apply(evaluate_case, axis=1, result_type=\"expand\")\n",
        "\n",
        "# Calculate overall accuracy for specialists (using predicted labels vs. evaluated correctness)\n",
        "accuracy = df['Specialist Score'].mean() * 100\n",
        "\n",
        "# Calculate overall question quality score\n",
        "average_question_quality = df['Question Quality Score'].mean()\n",
        "\n",
        "# Save the results to a new CSV\n",
        "# df.to_csv(\"/teamspace/studios/this_studio/evaluated_results_with_question_quality_first_20.csv\", index=False)\n",
        "\n",
        "# Output evaluation scores\n",
        "print(f\"✅ Evaluation complete for the first 30 rows. Overall specialist accuracy: {accuracy:.2f}%\")\n",
        "print(f\"✅ Average question quality score: {average_question_quality:.2f}\")\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
