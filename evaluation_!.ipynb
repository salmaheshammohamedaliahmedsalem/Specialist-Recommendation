{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TEwNmnTXm0uc"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM\n",
        "import torch\n",
        "from huggingface_hub import login\n",
        "login()\n",
        "# Load model and tokenizer for the medical expert LLM\n",
        "model_name = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, use_auth_token=True)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"auto\", torch_dtype=\"auto\", use_auth_token=True)\n",
        "\n",
        "llm = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
        "\n",
        "# Function to assess question quality\n",
        "def evaluate_question_quality(dialogue):\n",
        "    prompt = f\"\"\"\n",
        "    You are a senior medical expert. Based on the following patient-doctor dialogue, rate the quality of the questions asked:\n",
        "    \"{dialogue}\"\n",
        "    The quality of the questions should be evaluated based on their clarity, relevance, and how well they help in diagnosing the patient.\n",
        "    Provide a score from 1 (poor) to 10 (excellent) and a brief explanation.\n",
        "    \"\"\"\n",
        "    response = llm(prompt, max_new_tokens=100, do_sample=False)[0]['generated_text']\n",
        "    return response.strip()\n",
        "\n",
        "# Function to assess the correctness of the assigned specialist\n",
        "def evaluate_specialist(dialogue, assigned_specialist):\n",
        "    prompt = f\"\"\"\n",
        "    You are a senior medical expert. Given the following patient-doctor dialogue, assess if the assigned specialist is correct:\n",
        "    Dialogue: \"{dialogue}\"\n",
        "    Assigned Specialist: {assigned_specialist}\n",
        "    Based on the patient's condition described in the dialogue, is this the correct specialist to be assigned? Answer with \"Correct\" or \"Incorrect\" and provide a brief explanation.\n",
        "    \"\"\"\n",
        "    response = llm(prompt, max_new_tokens=100, do_sample=False)[0]['generated_text']\n",
        "    return response.strip()\n",
        "\n",
        "# Function to compute evaluation score for each case\n",
        "def evaluate_case(row):\n",
        "    dialogue = row['Dialogue']\n",
        "    assigned_specialist = row['Assigned Specialist']\n",
        "\n",
        "    # Evaluate the question quality and specialist correctness\n",
        "    question_quality = evaluate_question_quality(dialogue)\n",
        "    specialist_evaluation = evaluate_specialist(dialogue, assigned_specialist)\n",
        "\n",
        "    # Generate a score for each\n",
        "    question_score = int(question_quality.split()[0]) if question_quality.split()[0].isdigit() else 0\n",
        "    specialist_score = 1 if \"Correct\" in specialist_evaluation else 0\n",
        "\n",
        "    # Return the results\n",
        "    return question_score, specialist_score, question_quality, specialist_evaluation\n",
        "\n",
        "# Load the dataset containing the cases\n",
        "df = pd.read_csv(\"/content/your_labeled_data.csv\")\n",
        "\n",
        "# Apply the evaluation function\n",
        "df[['Question Quality Score', 'Specialist Score', 'Question Quality Explanation', 'Specialist Evaluation']] = df.apply(evaluate_case, axis=1, result_type=\"expand\")\n",
        "\n",
        "# Calculate overall accuracy for specialists (using predicted labels vs. evaluated correctness)\n",
        "accuracy = df['Specialist Score'].mean() * 100\n",
        "\n",
        "# Calculate overall question quality score\n",
        "average_question_quality = df['Question Quality Score'].mean()\n",
        "\n",
        "# Save the results to a new CSV\n",
        "df.to_csv(\"/content/evaluated_results_with_question_quality.csv\", index=False)\n",
        "\n",
        "# Output evaluation scores\n",
        "print(f\"✅ Evaluation complete. Overall specialist accuracy: {accuracy:.2f}%\")\n",
        "print(f\"✅ Average question quality score: {average_question_quality:.2f}\")\n",
        "\n"
      ]
    }
  ]
}